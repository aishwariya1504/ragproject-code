{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required class\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader ##reads PDF files page by page\n",
    "import os ##helps to navigate folders and files                                                                                                                   ##Function to Load PDFs\n",
    "\n",
    "def load_medical_pdfs(pdf_folder_path):\n",
    "    print(pdf_folder_path)                                                                                                                                        documents=[]                                                                                                                                                                import os\n",
    "pdf_folder_path = \"../data/pdf\" \n",
    "\n",
    "pdf_files = []\n",
    "\n",
    "for file_name in os.listdir(pdf_folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_files.append(file_name)\n",
    "\n",
    "pdf_files                                                                                                                                                     ## load each pdf\n",
    "\n",
    "loader = PyPDFLoader(\"../data/pdf/Antenatal care guidelines.pdf\")\n",
    "pdf_docs = loader.load()\n",
    "print(pdf_docs)\n",
    "## to all all pdfs \n",
    "all_docs = []\n",
    "for f in os.listdir(\"../data/pdf\"):\n",
    "    if f.endswith(\".pdf\"):\n",
    "        all_docs.extend(PyPDFLoader(os.path.join(\"../data/pdf\", f)).load())\n",
    "        print(all_docs)                                 \n",
    "     documents.extend(pdf_docs)\n",
    "print(len(documents))\n",
    "print(f\"Total pages loaded: {len(documents)}\")\n",
    "print(documents[0])                                                                                                                              documents = []\n",
    "\n",
    "for file_name in os.listdir(pdf_folder_path):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(pdf_folder_path, file_name))\n",
    "        docs = loader.load()\n",
    "\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = file_name\n",
    "\n",
    "        documents.extend(docs)\n",
    "        documents[0].metadata                                                                                                                  print(f\"Total pages loaded: {len(documents)}\")                                                                                                from collections import Counter\n",
    "\n",
    "pdf_summary = Counter(doc.metadata[\"source\"] for doc in documents)\n",
    "\n",
    "for pdf, pages in pdf_summary.items():\n",
    "    print(f\"{pdf} → {pages} pages\")\n",
    "print(f\"Total pages loaded: {len(docs)}\")\n",
    "print(docs[0])                                                                                                                                               !pip install langchain-text-splitters                                                                                                                       from langchain_text_splitters import RecursiveCharacterTextSplitter                                                              text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    \n",
    "    chunk_overlap=200\n",
    ")\n",
    " chunks = text_splitter.split_documents(documents)                                                                      print(f\"Number of chunks created: {len(chunks)}\") \n",
    "print(chunks[0].page_content[:500])                                                                 \n",
    "print(chunks[0].page_content[-200:]) \n",
    "print(chunks[1].page_content[:200])                                                                                                                       from sentence_transformers import SentenceTransformer \n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Model loaded successfully\")                                                                                                                            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS                                                                                            embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")vectorstore.save_local(\"faiss_index\")\n",
    "uery = \"How is diabetes treated?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content[:300])\n",
    "    print(doc.metadata)\n",
    "    print(\"-\" * 40)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")faiss_db = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")faiss_db.save_local(\"faiss_index\")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "chroma_db.persist()                                                                                                                                              query = \"How can we take care a mother after a child birth?\" \n",
    "\n",
    "results = faiss_db.similarity_search(query, k=3)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content[:200])\n",
    "    print(doc.metadata)\n",
    "    print(\"-\" * 40)                                                                                                                                                        pip install langchain langchain-community openai  \n",
    " from langchain_openai import ChatOpenAI                                                                                                                                      retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "## inspect retrievered documents\n",
    "retrieved_docs = retriever.invoke(\n",
    "    \"What are the treatments for diabetes?\"\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Doc {i+1}\")\n",
    "    print(doc.page_content[:200])\n",
    "    print(doc.metadata)\n",
    "    print(\"-\" * 40)\n",
    "## duplicate and noise removal\n",
    " def remove_duplicates(docs):\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        content = doc.page_content.strip()\n",
    "        if content not in seen:\n",
    "            unique_docs.append(doc)\n",
    "            seen.add(content)\n",
    "\n",
    "    return unique_docs\n",
    "\n",
    "\n",
    "clean_docs = remove_duplicates(retrieved_docs)\n",
    "def filter_by_source(docs, allowed_sources):\n",
    "    return [\n",
    "        doc for doc in docs\n",
    "        if doc.metadata.get(\"source\") in allowed_sources\n",
    "    ]\n",
    "\n",
    "\n",
    "trusted_docs = filter_by_source(\n",
    "    clean_docs,\n",
    "    allowed_sources=[\n",
    "        \"who_guidelines.pdf\",\n",
    "        \"clinical_protocols.pdf\"\n",
    "    ]\n",
    ")\n",
    "def build_context(docs, max_chars=3000):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        if len(context) + len(doc.page_content) <= max_chars:\n",
    "            context += doc.page_content + \"\\n\\n\"\n",
    "    return context\n",
    "\n",
    "\n",
    "final_context = build_context(trusted_docs)\n",
    "def safety_check(query):\n",
    "    restricted_terms = [\"dosage\", \"prescription\", \"self-medication\"]\n",
    "    for term in restricted_terms:\n",
    "        if term in query.lower():\n",
    "            return False\n",
    "    return True\n",
    "if not safety_check(query):\n",
    "    print(\"⚠️ This question requires a medical professional.\")\n",
    "chatbot_prompt = f\"\"\"\n",
    "You are a healthcare assistant chatbot.\n",
    "Answer ONLY from the provided context.\n",
    "If not found, say you don't know.\n",
    "\n",
    "Context:\n",
    "{final_context}\n",
    "\n",
    "Answer the user's question clearly and safely.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
